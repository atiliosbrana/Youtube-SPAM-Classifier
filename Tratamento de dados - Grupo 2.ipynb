{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Spam Collection v. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grupo 2\n",
    "Alberto Atilio Sbrana Junior\n",
    "<br>\n",
    "Luiz Barreto Pedro de Alcântara\n",
    "<br>\n",
    "Priscila Portela Costa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trata-se de um problema de classificação binária sobre comentários de vídeos no Youtube.\n",
    "<br>\n",
    "Há no total 5 arquivos, separados por artista:\n",
    "- Psy\n",
    "- Katy Perry\n",
    "- LMFAO\n",
    "- Eminem\n",
    "- Shakira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #manipulação de dataframes\n",
    "import numpy as np #manipulacao matricial\n",
    "import seaborn as sns #visualização\n",
    "import matplotlib.pyplot as plt #visualização\n",
    "\n",
    "# importa algumas biblioteca para plotar dados em 3D        \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Rectangle\n",
    "from pylab import *\n",
    "\n",
    "import scipy.optimize  #otimizacao de parametros\n",
    "\n",
    "import ML_library #biblioteca criada pelo grupo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos carregar cada um dos arquivos em um _dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psy = pd.read_csv('Youtube 01-comments Psy.csv')\n",
    "df_kat = pd.read_csv('Youtube 04-comments KatyPerry.csv')\n",
    "df_lma = pd.read_csv('Youtube 07-comments LMFAO.csv')\n",
    "df_emi = pd.read_csv('Youtube 08-comments Eminem.csv')\n",
    "df_sha = pd.read_csv('Youtube 09-comments Shakira.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos considerar que um modelo melhora ainda mais quando há uma quantidade maior de dados e consideraremos que todos os comentários são em inglês, vamos unir todos os _datasets_ e acrescentar uma nova coluna que indica em qual vídeo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por se tratar de análise de texto, é considerado boa prática realizar as seguintes transformações:\n",
    "- Todas as palavras para minúsculo\n",
    "- Remoção de caracteres especiais\n",
    "- Remoção de stop words\n",
    "- Igualar textos de contrações de palavras e.g 'plz' = 'please'\n",
    "- Remoção de emojis\n",
    "- Strings bizarras e.g \\ufeff\n",
    "\n",
    "Também temos uma variável horária, então precisamos transformar esse dado para que o modelo entenda que o valor hour=23 está próximo do valor hour=0, o que não se reflete quando tratamos essa coluna como se fosse um valor inteiro.\n",
    "<br>\n",
    "Uma prática comum é usar transformações trigonométricos para transformação horária."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos unir os _datasets_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psy['origin'] = 'psy'\n",
    "df_kat['origin'] = 'kat'\n",
    "df_lma['origin'] = 'lma'\n",
    "df_emi['origin'] = 'emi'\n",
    "df_sha['origin'] = 'sha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start = df_psy.append([df_kat, df_lma, df_emi, df_sha]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criação de novas colunas: ano, data e has_date (booleana, para o caso do dataset do eminem)\n",
    "df_start['DATE'] = pd.to_datetime(df_start['DATE'],infer_datetime_format=True)\n",
    "df_start['comment_year'] = df_start['DATE'].dt.year\n",
    "df_start['comment_hour'] = df_start['DATE'].dt.hour\n",
    "df_start['has_date'] = np.where(df_start['comment_year'] == 1969, False, True)\n",
    "\n",
    "df_start['comment_len'] = df_start['CONTENT'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do dataset unido: 1956 registros\n"
     ]
    }
   ],
   "source": [
    "print('Tamanho do dataset unido: {} registros'.format(len(df_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>origin</th>\n",
       "      <th>comment_year</th>\n",
       "      <th>comment_hour</th>\n",
       "      <th>has_date</th>\n",
       "      <th>comment_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07 06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07 12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08 17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09 08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10 16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                 DATE                                            CONTENT  \\\n",
       "0 2013-11-07 06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1 2013-11-07 12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2 2013-11-08 17:34:21             just for test I have to say murdev.com   \n",
       "3 2013-11-09 08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4 2013-11-10 16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS origin  comment_year  comment_hour  has_date  comment_len  \n",
       "0      1    psy        2013.0           6.0      True           56  \n",
       "1      1    psy        2013.0          12.0      True          166  \n",
       "2      1    psy        2013.0          17.0      True           38  \n",
       "3      1    psy        2013.0           8.0      True           48  \n",
       "4      1    psy        2013.0          16.0      True           39  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_start.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como já utilizamos a coluna de data, vamos excluí-la do _dataset_ e remover eventuais duplicados.\n",
    "<br>\n",
    "Vamos excuir também a coluna COMMENT_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start.drop(['DATE', 'COMMENT_ID'], axis=1, inplace=True)\n",
    "df_start.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>origin</th>\n",
       "      <th>comment_year</th>\n",
       "      <th>comment_hour</th>\n",
       "      <th>has_date</th>\n",
       "      <th>comment_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adam riyati</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>psy</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             AUTHOR                                            CONTENT  CLASS  \\\n",
       "0         Julius NM  Huh, anyway check out this you[tube] channel: ...      1   \n",
       "1       adam riyati  Hey guys check out my new channel and our firs...      1   \n",
       "2  Evgeny Murashkin             just for test I have to say murdev.com      1   \n",
       "3   ElNino Melendez   me shaking my sexy ass on my channel enjoy ^_^ ﻿      1   \n",
       "4            GsMega            watch?v=vtaRGgvGtWQ   Check this out .﻿      1   \n",
       "\n",
       "  origin  comment_year  comment_hour  has_date  comment_len  \n",
       "0    psy        2013.0           6.0      True           56  \n",
       "1    psy        2013.0          12.0      True          166  \n",
       "2    psy        2013.0          17.0      True           38  \n",
       "3    psy        2013.0           8.0      True           48  \n",
       "4    psy        2013.0          16.0      True           39  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_start.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos o dataset inicial, vamos separar em _features_ (X) e _target_ (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CLASS  counts\n",
       "0      0     946\n",
       "1      1     981"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_start.groupby('CLASS').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que a coluna alvo possui uma quantidade de valores balanceados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dicionario pra substituicao de contracoes, girias, typos, stemming\n",
    "word_dict = {\n",
    "    'pls': 'please',\n",
    "    'plz': 'please',\n",
    "    'plizz': 'please',\n",
    "    'pplease': 'please',\n",
    "    'pleassssssssssssssss': 'please',\n",
    "    'dis': 'this',\n",
    "    'yt': 'youtube',\n",
    "    'you[tube]': 'youtube',\n",
    "    'instagraml': 'instagram',\n",
    "    'facebook-page': 'facebook',\n",
    "    'wiredo': 'wierdo',\n",
    "    'vid': 'video',\n",
    "    'videoeo': 'video',\n",
    "    'sub': 'subscribe',\n",
    "    'thx': 'thanks',\n",
    "    'suscribe': 'subscribe',\n",
    "    'allot': 'lot',\n",
    "    'wat': 'what',\n",
    "    'should.d': 'should',\n",
    "    'ilove': 'love',\n",
    "    'likesubscribescribe': 'subscribe',\n",
    "    'subscribescribe': 'subscribe',\n",
    "    'anyoutubehing' : 'youtube',\n",
    "    'itttttttt': 'it',\n",
    "    'im': 'i',\n",
    "    'channelthanks': 'channel'\n",
    "}\n",
    "\n",
    "#lista de stop workds da biblioteca NLTK\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
    "              'you','you are','you have','you will',\"you had\",'your','yours','yourself','yourselves',\n",
    "              'he','him','his','himself','she',\"she is\",'her','hers','herself','it',\"it is\",'its',\n",
    "              'itself','they','them','their','theirs','themselves','what','which','who','whom','this','that',\n",
    "              \"that will\",'these','those','am','is','are','was','were','be','been','being','have',\n",
    "              'has','had','having','do','does','did','doing','a','an','the','and','but','if',\n",
    "              'or','because','as','until','while','of','at','by','for','with','about','against','between',\n",
    "              'into','through','during','before','after','above','below','to','from','up','down',\n",
    "              'in','out','on','off','over','under','again','further','then','once','here','there',\n",
    "              'when','where','why','how','all','any','both','each','few','more','most','other',\n",
    "              'some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can',\n",
    "              'will','just','don',\"do not\",'should',\"should have\",'now','d','ll','m','o','re','ve','y','ain',\n",
    "              'aren',\"are not\",'couldn',\"could not\",'didn',\"did not\",'doesn',\"does not\",'hadn',\"had not\",\n",
    "              'hasn',\"has not\",'haven',\"have not\",'isn',\"is not\",'ma','mightn',\"might not\",'mustn',\"must not\",\n",
    "              'needn',\"need not\",'shan',\"shall not\",'shouldn',\"should not\",'wasn',\"was not\",'weren',\"were not\",\n",
    "              'won',\"will not\",'wouldn',\"would not\", 'ill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_dados(df, treatment_type):\n",
    "    \n",
    "    #transformação trigonometrica da coluna hour\n",
    "    df['sin_time'] = np.sin(2*np.pi*df['comment_hour']/24)\n",
    "    df['cos_time'] = np.cos(2*np.pi*df['comment_hour']/24)\n",
    "        \n",
    "    #criar dummy pra cada origem\n",
    "    df = pd.get_dummies(df, columns=['origin', 'has_date'])\n",
    "    \n",
    "    #novas feature\n",
    "    df['has_www'] = df['CONTENT'].str.contains(pat = 'www') \n",
    "    df['has_http'] = df['CONTENT'].str.contains(pat = 'http')\n",
    "    df['has_link'] = df['CONTENT'].str.contains(pat = 'watch\\?v=')   \n",
    "    df['has_website'] = (df['has_www'] == True) | (df['has_http'] == True) | (df['has_link'] == True)\n",
    "    \n",
    "    #tratamento de texto\n",
    "    df.drop('AUTHOR', axis=1, inplace=True)\n",
    "    df['CONTENT'] = df['CONTENT'].str.lower()\n",
    "    \n",
    "    column_list = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        content = row['CONTENT']\n",
    "        \n",
    "        #retirar emojis e caracteres especiais\n",
    "        regular_characters = \"abcdefghijklmnopqrstuvwxyz \" \n",
    "        content = ''.join(c for c in content if c in regular_characters)\n",
    "        \n",
    "        #transfoma texto em lista de palavras:\n",
    "        content = content.split()\n",
    "                \n",
    "        #substituir contracoes de palavras\n",
    "        for k, v in word_dict.items():\n",
    "            content = [c.replace(k, v) for c in content]\n",
    "        \n",
    "        #remover stop words\n",
    "        for sw in stop_words:\n",
    "            content = [c for c in content if sw != c]\n",
    "        \n",
    "        column_list.append(content)\n",
    "        \n",
    "    column_list = [word for word in column_list if len(word) > 0]\n",
    " \n",
    "    df['CONTENT'] = pd.Series(column_list)\n",
    "    \n",
    "    df.dropna(axis=0, subset=['CONTENT'], inplace=True)\n",
    "    df = df[df['CONTENT'] != '[]']\n",
    "    \n",
    "    #transformar cada palavra em uma feature\n",
    "    columns = []\n",
    "    for sublist in column_list:\n",
    "        for item in sublist:\n",
    "            columns.append(item) \n",
    "        \n",
    "    columns = set(columns)            \n",
    "    dict_list = []\n",
    "        \n",
    "    #valores em cada coluna\n",
    "    for index, row in df.iterrows():\n",
    "        content_words = row['CONTENT']\n",
    "        \n",
    "        bag_of_words_dict = {}       \n",
    "        for c in columns:\n",
    "            bag_of_words_dict[c] = 0\n",
    "        \n",
    "        for w in content_words:\n",
    "            if treatment_type == 'occurrency':\n",
    "                bag_of_words_dict[w] = 1\n",
    "            if treatment_type == 'frequency':\n",
    "                bag_of_words_dict[w] += 1\n",
    "            if treatment_type == 'tf-idf':         \n",
    "                bag_of_words_dict[w] +=1\n",
    "                \n",
    "        if treatment_type == 'tf-idf': \n",
    "            for w in bag_of_words_dict:\n",
    "                bag_of_words_dict[w] = bag_of_words_dict[w] / len(content_words)\n",
    "        \n",
    "        dict_list.append(bag_of_words_dict)        \n",
    "     \n",
    "    df = df.assign(bag_words=dict_list)\n",
    "    bag_words = df['bag_words'].apply(pd.Series)\n",
    "    \n",
    "    if treatment_type == 'tf-idf':\n",
    "        for c in bag_words:\n",
    "            if bag_words[c].sum() > 0:\n",
    "                bag_words[c] = np.log(len(df)/bag_words[c].sum())*bag_words[c]\n",
    "            else:\n",
    "                bag_words[c] = 0\n",
    "        \n",
    "    df = pd.concat([df, bag_words], axis = 1)\n",
    "    \n",
    "    #remover coluna content\n",
    "    df.drop(['CONTENT','bag_words'], axis=1, inplace=True)\n",
    "    #remover possiveis nulos\n",
    "    df.dropna(inplace=True)\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_types = ['occurrency', 'frequency', 'tf-idf']\n",
    "treated_df_dict = {}\n",
    "\n",
    "for t in treatment_types:\n",
    "    df_treated = trata_dados(df_start, treatment_type=t)\n",
    "    treated_df_dict[t] = df_treated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, temos três _datasets_ com diferentes tipos de preprocessamento, todos salvos no dicionário `treated_df_dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos testados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serão utilizados os algoritmos ensinados em sala de aula:\n",
    "- K-Nearest Neighbors (K-NN)\n",
    "- Regressão Logística\n",
    "- Classificador Naive-Bayes\n",
    "- Redes Neurais Artificiais\n",
    "- Máquinas de Vetores de Suporte\n",
    "\n",
    "Caso seja necessário ou por motivos de visualização, reduziremos a dimensionalidade do nosso _dataset_ utilizando Análise de Componentes Principais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métrica de avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por se tratar de um classificador binário, é possível que ocorram 4 tipos de classificação:\n",
    "- Verdadeiros positivos\n",
    "- Verdadeiros negativos\n",
    "- Falsos positivos\n",
    "- Falsos negativos\n",
    "\n",
    "Obviamente, queremos o maior número de verdadeiros positivos e verdadeiros negativos possíveis em nosso modelo.\n",
    "<br>\n",
    "<br>\n",
    "Entretanto, no caso de um classificador de SPAM, pode-se escolher como uma boa métrica uma função que não só leve em conta a acurácia do modelo, mas também uma minimização do número de falsos positivos.\n",
    "<br>\n",
    "<br>\n",
    "Por exemplo, as gravadoras podem querer minimizar a quantidade de contários de fãs que possam ser erroneamente classificados como SPAM, ao custo de deixar um ou outro comentário SPAM ser classificado como legítimo.\n",
    "<br>\n",
    "<br>\n",
    "Portanto, utilizaremos o F1-Score, que é uma média harmônica entre o recall (quantos dos comentários SPAM foram corretamente classificados como SPAM) e a precisão (quantos dos comentários foram classificados corretamente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "F_1   = 2 * \\frac{precision * recall} {precision + recall}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idealmente, todo modelo deve ser avaliado em relação a um classificador _naive_ ou de ponto de partida, que pode, no nosso caso, ser descrito pelo comportamento de uma moeda.\n",
    "<br>\n",
    "Com isso, queremos que nosso classificador tenha a seguinte característica:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "F_1modelo > F_1naive,    \\text{para $F_1naive=0.5$}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separação de dados: _Stratified holdout_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_occur = treated_df_dict['occurrency']\n",
    "df_frequ = treated_df_dict['frequency']\n",
    "df_tfidf = treated_df_dict['tf-idf']\n",
    "\n",
    "#separacao X e y\n",
    "X_occur = df_occur.drop('CLASS', axis=1).values\n",
    "y_occur = df_occur['CLASS'].values\n",
    "\n",
    "X_frequ = df_frequ.drop('CLASS', axis=1).values\n",
    "y_frequ = df_frequ['CLASS'].values\n",
    "\n",
    "X_tfidf = df_tfidf.drop('CLASS', axis=1).values\n",
    "y_tfidf = df_tfidf['CLASS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros fixados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "# define a porcentagem de dados que irao compor o conjunto de treinamento\n",
    "pTrain = 0.8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação de valores de treino e teste para cada um dos _dataframes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm_occur = np.random.RandomState(randomSeed).permutation(range(len(y_occur)))\n",
    "idx_perm_frequ = np.random.RandomState(randomSeed).permutation(range(len(y_frequ)))\n",
    "idx_prem_tfidf = np.random.RandomState(randomSeed).permutation(range(len(y_tfidf)))\n",
    "\n",
    "#ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "X2_occur, Y2_occur = X_occur[idx_perm_occur, :], y_occur[idx_perm_occur]\n",
    "X2_frequ, Y2_frequ = X_frequ[idx_perm_frequ, :], y_frequ[idx_perm_frequ]\n",
    "X2_tfidf, Y2_tfidf = X_tfidf[idx_prem_tfidf, :], y_tfidf[idx_prem_tfidf]\n",
    "\n",
    "# obtem os indices dos dados da particao de treinamento e da particao de teste\n",
    "train_index_occur, test_index_occur = ML_library.stratified_holdOut(Y2_occur, pTrain)\n",
    "train_index_frequ, test_index_frequ = ML_library.stratified_holdOut(Y2_frequ, pTrain)\n",
    "train_index_tfidf, test_index_tfidf = ML_library.stratified_holdOut(Y2_tfidf, pTrain)\n",
    "\n",
    "#datasets de treino e teste\n",
    "#occurency\n",
    "X_train_occur, X_test_occur = X2_occur[train_index_occur, :], X2_occur[test_index_occur, :];\n",
    "Y_train_occur, Y_test_occur = Y2_occur[train_index_occur], Y2_occur[test_index_occur];\n",
    "\n",
    "#frequency\n",
    "X_train_frequ, X_test_frequ = X2_frequ[train_index_frequ, :], X2_frequ[test_index_frequ, :];\n",
    "Y_train_frequ, Y_test_frequ = Y2_frequ[train_index_frequ], Y2_frequ[test_index_frequ];\n",
    "\n",
    "#tf-idf\n",
    "X_train_tfidf, X_test_tfidf = X2_tfidf[train_index_tfidf, :], X2_tfidf[test_index_tfidf, :];\n",
    "Y_train_tfidf, Y_test_tfidf = Y2_tfidf[train_index_tfidf], Y2_tfidf[test_index_tfidf];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separação de dados para curva de aprendizado\n",
    "train_index_occur, val_index_occur = ML_library.stratified_holdOut(Y_train_occur, pTrain)\n",
    "train_index_frequ, val_index_frequ = ML_library.stratified_holdOut(Y_train_frequ, pTrain)\n",
    "train_index_tfidf, val_index_tfidf = ML_library.stratified_holdOut(Y_train_tfidf, pTrain)\n",
    "#occurrency\n",
    "X_train_v_occur, X_val_occur = X_train_occur[train_index_occur, :], X_train_occur[val_index_occur, :]\n",
    "Y_train_v_occur, Y_val_occur = Y_train_occur[train_index_occur], Y_train_occur[val_index_occur]\n",
    "#frequency\n",
    "X_train_v_frequ, X_val_frequ = X_train_frequ[train_index_frequ, :], X_train_frequ[val_index_frequ, :]\n",
    "Y_train_v_frequ, Y_val_frequ = Y_train_frequ[train_index_frequ], Y_train_frequ[val_index_frequ]\n",
    "#tf-idf\n",
    "X_train_v_tfidf, X_val_tfidf = X_train_tfidf[train_index_tfidf, :], X_train_tfidf[val_index_tfidf, :]\n",
    "Y_train_v_tfidf, Y_val_tfidf = Y_train_tfidf[train_index_tfidf], Y_train_tfidf[val_index_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A escolha do modelo será feita utilizando os _datasets_ de treino e validação e, por fim, serão testados no _dataset_ de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occurrency\n",
    "np.savetxt('X_occurrency_train.csv', X_train_v_occur, delimiter=\",\")\n",
    "np.savetxt('X_occurrency_validation.csv', X_val_occur, delimiter=\",\")\n",
    "np.savetxt('X_occurrency_test.csv', X_test_occur, delimiter=\",\") \n",
    "\n",
    "np.savetxt('Y_occurrency_train.csv', Y_train_v_occur, delimiter=\",\") \n",
    "np.savetxt('Y_occurrency_validation.csv', Y_val_occur, delimiter=\",\") \n",
    "np.savetxt('Y_occurrency_test.csv', Y_test_occur, delimiter=\",\") \n",
    "\n",
    "#frequency\n",
    "np.savetxt('X_frequency_train.csv', X_train_v_frequ, delimiter=\",\")\n",
    "np.savetxt('X_frequency_validation.csv', X_val_frequ, delimiter=\",\")\n",
    "np.savetxt('X_frequency_test.csv', X_test_frequ, delimiter=\",\") \n",
    "\n",
    "np.savetxt('Y_frequency_train.csv', Y_train_v_frequ, delimiter=\",\") \n",
    "np.savetxt('Y_frequency_validation.csv', Y_val_frequ, delimiter=\",\") \n",
    "np.savetxt('Y_frequency_test.csv', Y_test_frequ, delimiter=\",\") \n",
    "\n",
    "#tf-idf\n",
    "np.savetxt('X_tfidf_train.csv', X_train_v_tfidf, delimiter=\",\")\n",
    "np.savetxt('X_tfidf_validation.csv', X_val_tfidf, delimiter=\",\")\n",
    "np.savetxt('X_tfidf_test.csv', X_test_tfidf, delimiter=\",\") \n",
    "\n",
    "np.savetxt('Y_tfidf_train.csv', Y_train_v_tfidf, delimiter=\",\") \n",
    "np.savetxt('Y_tfidf_validation.csv', Y_val_tfidf, delimiter=\",\") \n",
    "np.savetxt('Y_tfidf_test.csv', Y_test_tfidf, delimiter=\",\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
